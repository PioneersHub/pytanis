{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import math\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import tomli\n",
    "import numpy as np\n",
    "import structlog\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_context('poster')\n",
    "sns.set(rc={'figure.figsize': (16, 9.)})\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 120)\n",
    "pd.set_option('display.max_columns', 120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the logging level\n",
    "logging.basicConfig(level=logging.INFO, stream=sys.stdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytanis\n",
    "from pytanis import GSheetsClient, PretalxClient\n",
    "from pytanis.pretalx import subs_as_df, reviews_as_df, speakers_as_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Be aware that this notebook might only run with the following version\n",
    "pytanis.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import event-specific settings to don't have them here in the notebook\n",
    "with open('config.toml', 'rb') as fh:\n",
    "    cfg = tomli.load(fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(subs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretalx_client = PretalxClient(blocking=True)\n",
    "subs_count, subs = pretalx_client.submissions(cfg['event_name'], params={'questions': 'all'})\n",
    "spkrs_count, spkrs = pretalx_client.speakers(cfg['event_name'], params={'questions': 'all'})\n",
    "revs_count, revs = pretalx_client.reviews(cfg['event_name'])\n",
    "subs, revs, spkrs = list(subs), list(revs), list(spkrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subs_df = subs_as_df(subs, with_questions=True)\n",
    "revs_df = reviews_as_df(revs)\n",
    "spkrs_df = speakers_as_df(spkrs, with_questions=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Speaker Statistics\n",
    "\n",
    "Determine the number of speakers per company."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group spkrs_df by \"Q: Company / Institute\" and count the number of submission and return a dataframe\n",
    "company_counts = spkrs_df.groupby('Q: Company / Institute').size().reset_index(name='count')\n",
    "\n",
    "# Sort the dataframe by the count of submissions\n",
    "company_counts = company_counts.sort_values('count', ascending=False)\n",
    "\n",
    "# rename \"Q: Company / Institute\" to \"company_name\"\n",
    "company_counts = company_counts.rename(columns={'Q: Company / Institute': 'company_name'})\n",
    "\n",
    "# company_counts.to_csv('company_counts.csv', index=False)\n",
    "company_counts.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from difflib import get_close_matches\n",
    "\n",
    "# Step 0: Create a dictionary to map similar names to a consistent name\n",
    "company_names = company_counts['company_name'].unique()\n",
    "company_name_map = {}\n",
    "for company_name in company_names:\n",
    "    company_names_exclude_name = [name for name in company_names if name != company_name]\n",
    "    \n",
    "    # except handling company name\n",
    "    lookup_name = company_name.strip()\n",
    "\n",
    "    # do lookup without company form\n",
    "    if lookup_name.endswith('GmbH'):\n",
    "        lookup_name = lookup_name[:-4]\n",
    "    matches = get_close_matches(lookup_name, company_names_exclude_name, n=1, cutoff=.8)  # Adjust the cutoff as needed\n",
    "    if matches:\n",
    "        company_name_map[company_name] = matches[0]\n",
    "\n",
    "    # do lookup without company form\n",
    "    if not lookup_name.endswith('GmbH'):\n",
    "        lookup_name = f'{lookup_name} GmbH'\n",
    "    matches = get_close_matches(lookup_name, company_names_exclude_name, n=1, cutoff=.8)  # Adjust the cutoff as needed\n",
    "    if matches:\n",
    "        company_name_map[company_name] = matches[0]\n",
    "\n",
    "    # do lookup with original company name\n",
    "    matches = get_close_matches(company_name, company_names_exclude_name, n=1, cutoff=.8)  # Adjust the cutoff as needed\n",
    "    if matches:\n",
    "        company_name_map[company_name] = matches[0]\n",
    "\n",
    "company_name_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# Step 1: Build an adjacency list to represent the graph\n",
    "adjacency_list = defaultdict(set)\n",
    "for key, value in company_name_map.items():\n",
    "    adjacency_list[key].add(value)\n",
    "    adjacency_list[value].add(key)\n",
    "\n",
    "# Step 2: Perform a DFS or BFS to find all connected components\n",
    "def find_groups(adjacency_list):\n",
    "    visited = set()\n",
    "    groups = []\n",
    "\n",
    "    def dfs(node, group):\n",
    "        visited.add(node)\n",
    "        group.append(node)\n",
    "        for neighbor in adjacency_list[node]:\n",
    "            if neighbor not in visited:\n",
    "                dfs(neighbor, group)\n",
    "\n",
    "    for node in adjacency_list:\n",
    "        if node not in visited:\n",
    "            group = []\n",
    "            dfs(node, group)\n",
    "            groups.append(group)\n",
    "    \n",
    "    return groups\n",
    "\n",
    "# Step 3: Get the grouped names\n",
    "company_name_groups = find_groups(adjacency_list)\n",
    "\n",
    "# Step 4: for each key in company_name_map find its group in company_name_groups\n",
    "company_name_group_map = {}\n",
    "for key, value in company_name_map.items():\n",
    "    for group in company_name_groups:\n",
    "        if key in group:\n",
    "            company_name_group_map[key] = group\n",
    "            break\n",
    "\n",
    "# Step 5: Assign the group names if available\n",
    "company_counts['company_name_grouping_arr'] = company_counts['company_name'].apply(lambda x: company_name_group_map.get(x, [x]))\n",
    "\n",
    "company_counts['company_name_grouping_str'] = company_counts['company_name_grouping_arr'].apply(lambda x: '[' + ', '.join(x) + ']')\n",
    "\n",
    "company_counts.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group \"company_counts\" by company_name_grouping column and sum the count column\n",
    "company_counts_by_group = company_counts.groupby('company_name_grouping_str')['count'].sum().reset_index()\n",
    "\n",
    "# Sort the dataframe by the count of submissions\n",
    "company_counts_by_group = company_counts_by_group.sort_values('count', ascending=False)\n",
    "\n",
    "# company_counts_by_group.to_csv('company_counts_by_group.csv', index=False)\n",
    "company_counts_by_group"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission Statistics\n",
    "Determine number of speakers/submissions per track and main track."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter for all submitted talks\n",
    "talks_df = subs_df.loc[subs_df['State'] == 'submitted']\n",
    "\n",
    "# TODO: fetch main tracks from pretalx\n",
    "main_tracks = ['PyData', 'PyCon', 'General']\n",
    "\n",
    "# TODO: fetch tracks from pretalx\n",
    "all_tracks = ['PyCon: MLOps & DevOps', 'PyCon: Programming & Software Engineering', 'PyCon: Python Language & Ecosystem', 'PyCon: Security', 'PyCon: Testing', 'PyCon: Django & Web', 'PyData: Data Handling & Engineering', 'PyData: Machine Learning & Deep Learning & Statistics', 'PyData: Natural Language Processing & Audio (incl. Generative AI NLP)', 'PyData: Computer Vision (incl. Generative AI CV)', 'PyData: Generative AI', 'PyData: Embedded Systems & Robotics', 'PyData: PyData & Scientific Libraries Stack', 'PyData: Visualisation & Jupyter', 'PyData: Research Software Engineering', 'General: Community & Diversity', 'General: Education, Career & Life', 'General: Ethics & Privacy', 'General: Infrastructure - Hardware & Cloud', 'General: Rust', 'General: Others']\n",
    "\n",
    "# all available submission types\n",
    "submission_types = talks_df['Submission type'].unique()\n",
    "\n",
    "# all available expertise levels\n",
    "expertise_levels = list(talks_df['Q: Expected audience expertise: Domain'].unique()) + list(talks_df['Q: Expected audience expertise: Python'].unique())\n",
    "expertise_levels = list(set(expertise_levels))\n",
    "\n",
    "# all expertise categories\n",
    "expertise_categories = ['Q: Expected audience expertise: Python', 'Q: Expected audience expertise: Domain']\n",
    "\n",
    "# create an dataframe with 'all_tracks' and all 'submission_types' as rows\n",
    "tracks_df = pd.DataFrame(all_tracks, columns=['Track'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All independent of submission type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group submittaded talks by track and count the number of submissions\n",
    "talks_quantification_by_domain_expertise = talks_df.groupby(['Track', 'Q: Expected audience expertise: Domain']).size().unstack(fill_value=0)\n",
    "talks_quantification_by_domain_expertise = tracks_df.join(talks_quantification_by_domain_expertise, on='Track')\n",
    "talks_quantification_by_domain_expertise['Total'] = talks_quantification_by_domain_expertise[['None', 'Novice', 'Intermediate', 'Advanced']].sum(axis=1)\n",
    "talks_quantification_by_domain_expertise['Total %'] = (talks_quantification_by_domain_expertise['Total'] / talks_quantification_by_domain_expertise['Total'].sum() * 100).round(2)\n",
    "talks_quantification_by_domain_expertise['Main Track'] = talks_quantification_by_domain_expertise['Track'].apply(lambda x: x.split(':')[0] if ':' in x else x)\n",
    "talks_quantification_by_domain_expertise['Total % per Main Track'] = talks_quantification_by_domain_expertise.groupby('Main Track')['Total'].transform(lambda x: (x / x.sum() * 100).round(2))\n",
    "\n",
    "# reorder columns\n",
    "talks_quantification_by_domain_expertise = talks_quantification_by_domain_expertise[['Main Track', 'Track', 'Total', 'Total %', 'Total % per Main Track', 'None', 'Novice', 'Intermediate', 'Advanced']]\n",
    "talks_quantification_by_python_expertise = talks_df.groupby(['Track', 'Q: Expected audience expertise: Python']).size().unstack(fill_value=0)\n",
    "talks_quantification_by_python_expertise = tracks_df.join(talks_quantification_by_python_expertise, on='Track')\n",
    "talks_quantification_by_python_expertise['Main Track'] = talks_quantification_by_python_expertise['Track'].apply(lambda x: x.split(':')[0] if ':' in x else x)\n",
    "talks_quantification_by_python_expertise = talks_quantification_by_python_expertise[['Main Track', 'Track', 'None', 'Novice', 'Intermediate', 'Advanced']]\n",
    "\n",
    "# join talks_quantification_by_domain_expertise and talks_quantification_by_python_expertise and keep add a group column name fir the expertise level\n",
    "talks_quantification = pd.merge(talks_quantification_by_domain_expertise, talks_quantification_by_python_expertise, on=['Main Track', 'Track'], how='outer')\n",
    "\n",
    "talks_quantification.columns = pd.MultiIndex.from_tuples([\n",
    "    ('', col) if (col == 'Track') | (col == 'Total') | (col == 'Total %') | (col == 'Total % per Main Track') | (col == 'Main Track') else \n",
    "    ('Expected Domain Expertise by Audience', col.rstrip(\"_xy\")) if col.endswith('_x') else \n",
    "    ('Expected Python Expertise by Audience', col.rstrip(\"_xy\")) \n",
    "    for col in talks_quantification.columns\n",
    "])\n",
    "\n",
    "# fill NaN values with 0\n",
    "talks_quantification.fillna(0, inplace=True)\n",
    "\n",
    "talks_quantification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compress overall table for plotting\n",
    "talks_quantification_condensed = talks_quantification.copy()\n",
    "\n",
    "talks_quantification_condensed['', 'Expected Domain Expertise by Audience'] = talks_quantification_condensed['Expected Domain Expertise by Audience'].to_numpy().tolist()\n",
    "talks_quantification_condensed['', 'Expected Python Expertise by Audience'] = talks_quantification_condensed['Expected Python Expertise by Audience'].to_numpy().tolist()\n",
    "\n",
    "talks_quantification_condensed = talks_quantification_condensed.drop(columns=['Expected Domain Expertise by Audience', 'Expected Python Expertise by Audience'], level=0)\n",
    "talks_quantification_condensed.columns = talks_quantification_condensed.columns.droplevel(0)\n",
    "\n",
    "# helper functions for plotting\n",
    "def cell_histogram_with_labels(values, global_max_value=None):\n",
    "    max_value = max(values) if global_max_value is None else global_max_value  # Maximalwert für Skalierung\n",
    "    bar_heights = [100 / len(values)] * len(values)  # Gleichmäßige Balkenhöhen (in Prozent)\n",
    "    bars = \"\"\n",
    "    labels = ['None', 'Novice', 'Intermediate', 'Advanced']\n",
    "    for i, value in enumerate(values):\n",
    "        label = labels[i]\n",
    "        bar_width = (value / max_value) * 100 if max_value > 0 else 0  # Width\n",
    "        y_position = i * bar_heights[0]  # Y-Position of each bar\n",
    "        # Rechteck (Bar)\n",
    "        bars += f'<rect x=\"0\" y=\"{y_position}%\" width=\"{bar_width}%\" height=\"{bar_heights[0]}%\" style=\"fill:#d65f5f50;\" />'\n",
    "        # Text (Label)\n",
    "        bars += f'<text x=\"2\" y=\"{y_position + bar_heights[0] / 1.8}%\" dominant-baseline=\"middle\" font-size=\"10\" fill=\"black\">{label} ({int(value)})</text>'\n",
    "    \n",
    "    svg = f\"\"\"\n",
    "    <svg width=\"100\" height=\"50\" xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 100 50\">\n",
    "        {bars}\n",
    "    </svg>\n",
    "    \"\"\"\n",
    "    return svg\n",
    "\n",
    "def single_value_histogram(value, max_value):\n",
    "    # Calculate the width of the bar as a percentage\n",
    "    bar_width = (value / max_value) * 100 if max_value > 0 else 0\n",
    "    \n",
    "    # Generate the SVG\n",
    "    svg = f\"\"\"\n",
    "    <svg width=\"100\" height=\"50\" xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 100 20\">\n",
    "        <!-- Rectangle (Bar) -->\n",
    "        <rect x=\"0\" y=\"-20\" width=\"{bar_width}%\" height=\"80\" style=\"fill:#d65f5f50;\" />\n",
    "        <!-- Text (Label) -->\n",
    "        <text x=\"5\" y=\"15\" font-size=\"14\" fill=\"black\">{round(value, 2)}%</text>\n",
    "    </svg>\n",
    "    \"\"\"\n",
    "    return svg\n",
    "\n",
    "# Generate output\n",
    "title = f'All {int(talks_quantification_condensed['Total'].sum())} submitted talks, long talks and tutorials (excluding pending and withdrawn submissions) <br> ****'\n",
    "\n",
    "talks_quantification_condensed_styled = talks_quantification_condensed.style \\\n",
    "    .set_caption(title) \\\n",
    "    .set_table_styles([\n",
    "        {'selector': 'caption', 'props': [('font-family', 'Arial'), ('font-size', '20px'), ('font-weight', 'bold')]},\n",
    "        {'selector': 'th', 'props': [('font-family', 'Arial'), ('max-width', '160px')]}\n",
    "    ]) \\\n",
    "    .set_properties(**{'font-family': 'Arial'}) \\\n",
    "    .format({\n",
    "        ('Total'): '{:.0f}',\n",
    "        ('Total %'): lambda value: single_value_histogram(\n",
    "            value,\n",
    "            talks_quantification_condensed['Total %'].max()\n",
    "        ),\n",
    "        ('Total % per Main Track'): lambda value: single_value_histogram(\n",
    "            value,\n",
    "            talks_quantification_condensed['Total % per Main Track'].max()\n",
    "        ),\n",
    "        'Expected Domain Expertise by Audience': lambda values: cell_histogram_with_labels(\n",
    "            values,\n",
    "            np.concatenate(talks_quantification_condensed['Expected Domain Expertise by Audience'].to_numpy()).max()\n",
    "        ),\n",
    "        'Expected Python Expertise by Audience': lambda values: cell_histogram_with_labels(\n",
    "            values,\n",
    "            np.concatenate(talks_quantification_condensed['Expected Python Expertise by Audience'].to_numpy()).max()\n",
    "        ),\n",
    "    })\n",
    "\n",
    "talks_quantification_condensed_styled.to_html('talks_quantification.html', index=False, escape=False)\n",
    "\n",
    "talks_quantification_condensed_styled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Future Todos\n",
    "- Compare against historical events\n",
    "- Split by submission type\n",
    "- Make independent of submission type\n",
    "- Topic modelling and keyword freqzency on submissions\n",
    "- Fetch tracks from Pretalx automatically (no static list as above)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
